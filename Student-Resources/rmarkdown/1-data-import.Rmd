# Create Spark Context

The `sparklyr` package has a handy function for creating a Spark context. This differs from the method that is used by the `SparkR` package.


Let's create our Spark Session. In this code, we will design our Spark Session using Microsoft's Spark Session. 

```{r spark_context}

library(sparklyr)
library(dplyr)

conf <- spark_config()
conf$'sparklyr.shell.executor-memory' <- "1g"
conf$'sparklyr.shell.driver-memory' <- "1g"
conf$spark.executor.cores <- 3
conf$spark.executor.memory <- "12G"
conf$spark.yarn.am.cores  <- 1
conf$spark.yarn.am.memory <- "1G"
conf$spark.dynamicAllocation.enabled <- "false"

## our cluser has 2 D13V2, 50 gigs and 15 cores
## Let's try: 4 executors per node (8 total), 12 gigs each, and 3 cores each
## overhead = 10 gigs

sc <- spark_connect(master = "yarn-client", config = conf)

```

# Download Sample Data 

```{r download_data}

# download.file("https://alizaidi.blob.core.windows.net/training/sample_taxi.csv", "sample_taxi.csv")
# or the larger data!
download.file("http://alizaidi.blob.core.windows.net/training/taxi_large.csv", "taxi_large.csv")

wasb_taxi <- "/NYCTaxi/sample"
rxHadoopListFiles("/")
rxHadoopMakeDir(wasb_taxi)
rxHadoopCopyFromLocal("taxi_large.csv", wasb_taxi)
rxHadoopCommand("fs -cat /NYCTaxi/sample/taxi_large.csv | head")


```

Let's also download the sample data that we can work with locally.

```{r download_sample}
taxi_url <- "http://alizaidi.blob.core.windows.net/training/trainingData/manhattan_df.rds"
taxi_df  <- readRDS(gzcon(url(taxi_url)))
(taxi_df <- tbl_df(taxi_df))
```


# Import Data

To import data from csv files, we can use the `spark_read_csv` function, which is basically a wrapper for the `read.df` function using the __databricks.spark.csv__ package.

```{r import_csv}

taxi <- spark_read_csv(sc,
                       path = wasb_taxi,
                       "taxisample",
                       header = TRUE)


```

