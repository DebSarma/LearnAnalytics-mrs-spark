---
title: "Creating Spark Sessions"
output: 
  html_notebook:
    toc: true
    toc_depth: 2
    toc_float: true
editor_options: 
  chunk_output_type: inline
---

# Create Spark Context

In this session we will create a Spark Session using the `rxSparkConnect` function from the `RevoScaleR` package. This will a `scaleR` Spark Session and create some additional necessary paths for us.

```{r spark_context}

cc <- rxSparkConnect()
hive_tbl <- RxHiveData(table = "hivesampletable",
                       colInfo = list(clientid = list(type = "integer"),
                                      querytime = list(type = "character"),
                                      market = list(type = "factor"),
                                      deviceplatform = list(type = "factor")))
rxGetInfo(hive_tbl, getVarInfo = TRUE)
rxSummary(~querydwelltime, data = hive_tbl)
rxSparkDisconnect()
```

# Download Sample Data 

```{r download_data}

# download.file("https://alizaidi.blob.core.windows.net/training/sample_taxi.csv", "sample_taxi.csv")
# or the larger data!
download.file("http://alizaidi.blob.core.windows.net/training/taxi_large.csv", "taxi_large.csv")

wasb_taxi <- "/NYCTaxi/sample"
rxHadoopListFiles("/")
rxHadoopMakeDir(wasb_taxi)
rxHadoopCopyFromLocal("taxi_large.csv", wasb_taxi)
rxHadoopCommand("fs -cat /NYCTaxi/sample/taxi_large.csv | head")


```

Let's also download the sample data that we can work with locally.

```{r download_sample}
taxi_url <- "http://alizaidi.blob.core.windows.net/training/trainingData/manhattan_df.rds"
taxi_df  <- readRDS(gzcon(url(taxi_url)))
(taxi_df <- tbl_df(taxi_df))
```


# Import Data

To import data from csv files, we can use the `spark_read_csv` function, which is basically a wrapper for the `read.df` function using the __databricks.spark.csv__ package.

```{r import_csv}

taxi <- spark_read_csv(sc,
                       path = wasb_taxi,
                       "taxisample",
                       header = TRUE)


```

